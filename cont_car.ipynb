{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUXRwmE00meY"
   },
   "outputs": [],
   "source": [
    "from gym import make\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mqoh202l0yF7"
   },
   "outputs": [],
   "source": [
    "N_STEP = 2\n",
    "GAMMA = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Car2u_j11VyR"
   },
   "outputs": [],
   "source": [
    "def transform_state(state):\n",
    "    state = (np.array(state) + np.array((1.2, 0))) / np.array((1.8, 0.07))\n",
    "    result = []\n",
    "    result.extend(state)\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xAdQIPvL1Xix"
   },
   "outputs": [],
   "source": [
    "class Replay:\n",
    "    def __init__(self, replay_size):\n",
    "        self.rs = replay_size\n",
    "        self.arr = []\n",
    "        self.ind = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.arr) < self.rs:\n",
    "            self.arr.append(transition)\n",
    "        else:\n",
    "            self.arr[self.ind] = transition\n",
    "            self.ind = (self.ind + 1) % self.rs\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        res = random.sample(self.arr, batch_size)\n",
    "        states = np.array([tran[0] for tran in res])\n",
    "        actions = np.array([tran[1] for tran in res])\n",
    "        next_states = np.array([tran[2] for tran in res])\n",
    "        rewards = np.array([tran[3] for tran in res])\n",
    "        dones = np.array([tran[4] for tran in res])\n",
    "        dones = np.array([0 if done else 1 for done in dones])\n",
    "        return states, actions, next_states, rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVO6e6aB1ZJY"
   },
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_dim, \n",
    "                 action_dim,\n",
    "                 hd1=32,\n",
    "                 hd2=32, \n",
    "                 batch_size=100,\n",
    "                 replay_size=10000,\n",
    "                 update_rate=100):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.gamma = GAMMA ** N_STEP\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim, hd1),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(hd1, hd2),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(hd2, action_dim))\n",
    "        self.replay = Replay(replay_size)\n",
    "        self.bs = batch_size\n",
    "        self.Loss = torch.nn.MSELoss()\n",
    "        self.target = copy.deepcopy(self.model)\n",
    "        # self.optim = torch.optim.Adam(self.model.parameters(), lr=1)\n",
    "        self.lr = 0\n",
    "        self.ur = update_rate\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "    \n",
    "    def update(self, transition):\n",
    "        #         state, action, next_state, reward, done = transition\n",
    "        self.replay.add(transition)\n",
    "        if len(self.replay.arr) < self.bs:\n",
    "            return\n",
    "        states, actions, next_states, rewards, dones = self.replay.get(int(self.bs))\n",
    "        self.lr += 1\n",
    "        Q1 = self.get_probs(states)[np.arange(self.bs), actions]\n",
    "        Q2 = rewards + self.gamma * np.max(self.get_target(next_states).data.numpy(), 1) * dones\n",
    "        Q2 = Variable(torch.Tensor(Q2), requires_grad=True)\n",
    "        optim = torch.optim.Adam(self.model.parameters(), lr=1. / pow(self.lr, 0.8))\n",
    "\n",
    "        loss = self.Loss(Q1, Q2)\n",
    "        # self.optim.zero_grad()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        # self.optim.step()\n",
    "        optim.step()\n",
    "        if self.lr > self.ur:\n",
    "            self.update_target()\n",
    "\n",
    "    def get_target(self, state):\n",
    "        ns = Variable(torch.Tensor(state))\n",
    "        return self.target(ns)\n",
    "\n",
    "    def get_probs(self, state):\n",
    "        ns = Variable(torch.Tensor(state))\n",
    "        return self.model(ns)\n",
    "\n",
    "    def act(self, state, target=False):\n",
    "        res = torch.argmax(self.get_probs(state).data)\n",
    "        return int(res)\n",
    "            \n",
    "    def update_target(self):\n",
    "        self.target = copy.deepcopy(self.model)\n",
    "    \n",
    "    def save(self, path=None):\n",
    "        torch.save(self.model.state_dict(), \"agent\"+str(path)+\".pkl\")\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        self.model.load_state_dict(torch.load(\"agent\"+str(path)+\".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Z7InWvw1bHA"
   },
   "outputs": [],
   "source": [
    "def eps(eps_min, i, top=800, top2=400):\n",
    "    slope = (eps_min - 1.0) / top\n",
    "    if i < top:\n",
    "        return slope * i + 1.0\n",
    "    return eps_min\n",
    "    # return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILuNMu3P1dX1"
   },
   "outputs": [],
   "source": [
    "class DDQN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 state_dim, \n",
    "                 action_dim,\n",
    "                 hd1=24,\n",
    "                 hd2=24, \n",
    "                 batch_size=64,\n",
    "                 replay_size=10000,\n",
    "                 update_rate=0.5):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.gamma = GAMMA ** N_STEP\n",
    "        self.dqnA= torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim, hd1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd1, hd2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd2, action_dim))\n",
    "        \n",
    "        self.dqnB= torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim, hd1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd1, hd2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd2, action_dim))\n",
    "        self.lrA = 0\n",
    "        self.lrB = 0\n",
    "        self.replay = Replay(replay_size)\n",
    "        self.bs = batch_size\n",
    "        self.Loss = torch.nn.MSELoss()\n",
    "        self.ur = update_rate\n",
    "        # self.optimA = torch.optim.RMSprop(self.dqnA.parameters(), lr=1e-3)\n",
    "        # self.optimB = torch.optim.RMSprop(self.dqnB.parameters(), lr=1e-3)\n",
    "\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.dqnA(state)\n",
    "\n",
    "\n",
    "    def update(self, transition):\n",
    "        self.replay.add(transition)\n",
    "        if len(self.replay.arr) < self.bs:\n",
    "            return\n",
    "        states, actions, next_states, rewards, dones = self.replay.get(int(self.bs))\n",
    "        if np.random.rand() > self.ur:\n",
    "            self.lrA += 1\n",
    "            next_actions = torch.argmax(self.get_probsA(next_states), 1)\n",
    "            Q1 = self.get_probsA(states)[np.arange(self.bs), actions]\n",
    "            Q2 = rewards + self.gamma * self.get_probsB(next_states)[np.arange(self.bs), next_actions].detach().numpy() * dones\n",
    "            Q2 = Variable(torch.Tensor(Q2), requires_grad=True)\n",
    "            optimA = torch.optim.Adam(self.dqnA.parameters(), lr=1. / pow(self.lrA, 0.8))\n",
    "            loss = self.Loss(Q1, Q2)\n",
    "            optimA.zero_grad()\n",
    "            loss.backward()\n",
    "            optimA.step()\n",
    "        else:\n",
    "            self.lrB += 1\n",
    "            next_actions = torch.argmax(self.get_probsB(next_states), 1)\n",
    "            Q1 = self.get_probsB(states)[np.arange(self.bs), actions]\n",
    "            Q2 = rewards + self.gamma * self.get_probsA(next_states)[np.arange(self.bs), next_actions].detach().numpy() * dones\n",
    "            Q2 = Variable(torch.Tensor(Q2), requires_grad=True)\n",
    "            optimB = torch.optim.Adam(self.dqnB.parameters(), lr=1. / pow(self.lrB, 0.8))\n",
    "            loss = self.Loss(Q1, Q2)\n",
    "            optimB.zero_grad()\n",
    "            loss.backward()\n",
    "            optimB.step()\n",
    "    \n",
    "    def get_probsA(self, state):\n",
    "        ns = Variable(torch.Tensor(state))\n",
    "        return self.dqnA(ns)\n",
    "    \n",
    "    def get_probsB(self, state):\n",
    "        ns = Variable(torch.Tensor(state))\n",
    "        return self.dqnB(ns)     \n",
    "\n",
    "    def act(self, state, target=False):\n",
    "        res = torch.argmax(self.get_probsA(state).data)\n",
    "        return int(res)\n",
    "            \n",
    "    def save(self, path=None):\n",
    "        torch.save(self.dqnA.state_dict(), \"agentCarA\"+str(path)+\".pkl\")\n",
    "        torch.save(self.dqnB.state_dict(), \"agentCarB\"+str(path)+\".pkl\")\n",
    "\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        self.dqnA.load_state_dict(torch.load(\"agentCarA\"+str(path)+\".pkl\"))\n",
    "        self.dqnB.load_state_dict(torch.load(\"agentCarB\"+str(path)+\".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weow4UAd1gle"
   },
   "outputs": [],
   "source": [
    "env = make(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TWKOQBeA5dtX"
   },
   "outputs": [],
   "source": [
    "def act(x, n):\n",
    "    return np.array([-1. + 2. * x / n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi-kYulOLnOZ"
   },
   "outputs": [],
   "source": [
    "def trans_act(action):\n",
    "    return int(5 * (1. + action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "1B1LTioi6iFx",
    "outputId": "ec401d5e-eb6f-4b60-d164-9f5d1cdf870b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make(\"MountainCarContinuous-v0\")\n",
    "env.seed(420)\n",
    "n = 15\n",
    "ddqn = DDQN(state_dim=2, \n",
    "            action_dim=n,\n",
    "            hd1=12, \n",
    "            hd2=12,\n",
    "            update_rate=0.2)\n",
    "eps_min = 0.001\n",
    "rw = []\n",
    "episodes = 1200\n",
    "best = 0\n",
    "for i in range(episodes):\n",
    "    state = transform_state(env.reset())\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    reward_buffer = deque(maxlen=N_STEP)\n",
    "    state_buffer = deque(maxlen=N_STEP)\n",
    "    action_buffer = deque(maxlen=N_STEP)\n",
    "    while not done:\n",
    "        if random.random() < eps(eps_min, i, top=1000):\n",
    "        # if random.random() < eps_min:\n",
    "            action = np.random.randint(n)\n",
    "        else:\n",
    "            action = ddqn.act(state)\n",
    "        next_state, reward, done, _ = env.step(act(action, n))\n",
    "        next_state = transform_state(next_state)\n",
    "        total_reward += reward + next_state[1]\n",
    "        steps += 1\n",
    "        reward_buffer.append(reward)\n",
    "        state_buffer.append(state)\n",
    "        action_buffer.append(action)\n",
    "        if len(reward_buffer) == N_STEP:\n",
    "            ddqn.update((state_buffer[0],\n",
    "                         action_buffer[0], \n",
    "                         next_state, \n",
    "                         sum([(GAMMA ** i) * r for i, r in enumerate(reward_buffer)]), \n",
    "                         done))\n",
    "        state = next_state\n",
    "    if len(reward_buffer) == N_STEP:\n",
    "        rb = list(reward_buffer)\n",
    "        for k in range(1, N_STEP):\n",
    "            ddqn.update((state_buffer[k],\n",
    "                         action_buffer[k], \n",
    "                         next_state, sum([(GAMMA ** i) * r for i, r in enumerate(rb[k:])]),\n",
    "                         done))\n",
    "    \n",
    "    \n",
    "\n",
    "    rwc = 0\n",
    "    state = transform_state(env.reset())\n",
    "    \n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = ddqn.act(state)\n",
    "        next_state, reward, done, _ = env.step(act(action, n))\n",
    "        next_state = transform_state(next_state)\n",
    "        rwc += reward\n",
    "        steps += 1\n",
    "        state = next_state\n",
    "    if rwc > best:\n",
    "        ddqn.save(i)\n",
    "        best = rwc\n",
    "    rw.append(rwc)\n",
    "    if sum(rw[-50:])/50 > 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QEAGP793Ncpw",
    "outputId": "659f4912-3db3-4742-d081-08b04410cad0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.61666222222524"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make(\"MountainCarContinuous-v0\")\n",
    "total_reward = 0\n",
    "tdqn = DDQN(2, n, 12, 12)\n",
    "tdqn.load(2)\n",
    "for i in range(100):\n",
    "    state = transform_state(env.reset())\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = tdqn.act(state)\n",
    "        next_state, reward, done, _ = env.step(act(action, n))\n",
    "        next_state = transform_state(next_state)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        state = next_state\n",
    "    # print(i)\n",
    "tr = total_reward / 100\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdbZp7zCSCuT"
   },
   "outputs": [],
   "source": [
    "!rm agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOISVjgCSvxJ"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        state_dim = 2\n",
    "        action_dim = 15\n",
    "        hd1 = 12\n",
    "        hd2 = 12\n",
    "        self.dqnA= torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim, hd1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd1, hd2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd2, action_dim))\n",
    "        \n",
    "        self.dqnB= torch.nn.Sequential(\n",
    "            torch.nn.Linear(state_dim, hd1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd1, hd2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hd2, action_dim))\n",
    "\n",
    "\n",
    "        self.dqnA.load_state_dict(torch.load(\"agentCarA2.pkl\"))\n",
    "        self.dqnB.load_state_dict(torch.load(\"agentCarB2.pkl\"))\n",
    "\n",
    "    def get_probsA(self, state):\n",
    "        ns = Variable(torch.Tensor(state))\n",
    "        return self.dqnA(ns)\n",
    "    \n",
    "    def get_probsB(self, state):\n",
    "        ns = Variable(torch.Tensor(state))\n",
    "        return self.dqnB(ns)     \n",
    "\n",
    "    def act(self, state, target=False):\n",
    "        res = torch.argmax(self.get_probsA(state).data)\n",
    "        return int(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.59444000000585"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = make(\"MountainCarContinuous-v0\")\n",
    "total_reward = 0\n",
    "for i in range(100):\n",
    "    state = transform_state(env.reset())\n",
    "    steps = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = ag.act(state)\n",
    "        next_state, reward, done, _ = env.step(act(action, 15))\n",
    "        next_state = transform_state(next_state)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        state = next_state\n",
    "tr = total_reward / 100\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cont_car.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
